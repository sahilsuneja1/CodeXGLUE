# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import os
import argparse
import re
from io import BytesIO
import json

lits = json.load(open("literals.json"))

def process_string(token, special_chars={" ": "U+0020", ",": "U+002C"}):
    str_quote_options = ["'''", '"""', "'", '"']
    start_quote = ""
    end_quote = ""
    qualifier_regex = r"^[a-zA-Z]+"
    qualifier_match = re.search(qualifier_regex, token)
    # string qualifiers like 'r' for regex, 'f' for formatted string, 'b' for bytes, 'u' for unicode, etc (or combination of them)
    qualifier = "" if not qualifier_match else qualifier_match[0]
    # token string without qualifiers
    token_string = re.sub(qualifier_regex, "", token)
    # string literal without quotes
    str_lit = token_string
    for q in str_quote_options:
        if token_string.startswith(q):
            start_quote = q
            str_lit = str_lit[len(q) :]
            if token_string.endswith(q):
                end_quote = q
                str_lit = str_lit[: -len(q)]
            break
    # if start_quote in str_quote_options[:2]:
    #     return ""
    for sc in special_chars:
        str_lit = str_lit.replace(sc, special_chars[sc])
    return (
        f"{qualifier}{start_quote}<STR_LIT:{str_lit}>{end_quote}"
        if str_lit in lits['str']
        else f"{qualifier}{start_quote}<STR_LIT>{end_quote}"
    )

def tokenize_file(args, sample_filename):
    try:
        code = open(os.path.join(args.base_dir, sample_filename)).read()
        token_gen = tokenize(BytesIO(bytes(code, "utf8")).readline)
        out_tokens = []
        prev_eol = False
        for toknum, tokval, _, _, _ in token_gen:
            tokval = " ".join(tokval.split())
            if toknum == STRING:
                add_token = process_string(tokval)
                out_tokens.append(add_token)
                prev_eol = False
            elif toknum == NUMBER:
                if tokval in lits['num']:
                    out_tokens.append(f"<NUM_LIT:{tokval}>")
                else:
                    out_tokens.append(f"<NUM_LIT>")
                prev_eol = False
            elif toknum in [NEWLINE, NL]:
                if not prev_eol:
                    out_tokens.append("<EOL>")
                    prev_eol = True
            elif toknum in [COMMENT, INDENT, ENCODING, ENDMARKER] or len(tokval) == 0:
                continue
            else:
                out_tokens.append(tokval)
                prev_eol = False
        if out_tokens[0] == "<EOL>":
            out_tokens = out_tokens[1:]
        if out_tokens[-1] == "<EOL>":
            out_tokens = out_tokens[:-1]
    except Exception:
        out_tokens = []
    return out_tokens    


def tokenize_files(args, file_type):
    wf = open(os.path.join(args.output_dir, f"{args.dataset_split_type}.txt"), 'w')
    sample_files = os.listdir(args.base_dir)
    for sample_filename in sample_files:
        out_tokens = tokenize_file(args, sample_filename)
        out_tokens = ["<s>"] + out_tokens + ["</s>"]
        out = " ".join(out_tokens)
        wf.write(out+"\n")
    wf.close()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_dir", default="cobol_opensource_validation", type=str, 
                        help="The downloaded data path")
    parser.add_argument("--output_dir", default="token_completion", type=str, 
                        help="The output directory")
    parser.add_argument("--dataset_split_type", default="validate", type=str, 
                        help="split type of dataset (train, test, validate)")
    args = parser.parse_args()

    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)

    import pdb
    pdb.set_trace()
    tokenize_files(args)

if __name__ == "__main__":
    main()
